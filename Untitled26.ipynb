{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTHhMaQ9bu8OZphFLfhRkf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cetinkarasar/Hukuk_Asistani_MLOps/blob/main/Untitled26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xIceokWAYEqy",
        "outputId": "e39ed058-eb04-4908-823e-dbacf1393d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "changed 22 packages in 2s\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "# 1. Temel kütüphaneler ve Unsloth kurulumu\n",
        "!pip install -q --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -q --no-cache-dir \\\n",
        "    trl==0.18.2 \\\n",
        "    xformers \\\n",
        "    peft \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-text-splitters \\\n",
        "    sentence-transformers \\\n",
        "    faiss-cpu \\\n",
        "    pypdf \\\n",
        "    streamlit \\\n",
        "    huggingface-hub==0.34.0\n",
        "\n",
        "# 2. LocalTunnel\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eksik olan ve hata veren parçalayıcı paketini kuruyoruz\n",
        "!pip install -q langchain-text-splitters langchain --upgrade"
      ],
      "metadata": {
        "id": "8OUNdQlMb1rq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import os\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# --- AYARLAR ---\n",
        "HF_REPO_NAME = \"Cetin003/hukuk_model_tck_v1_lora\"\n",
        "PDF_PATH = \"data/tck.pdf\"\n",
        "FAISS_INDEX_PATH = \"faiss_tck_kutuphanesi\"\n",
        "EMBEDDING_MODEL_NAME = \"distiluse-base-multilingual-cased-v1\"\n",
        "\n",
        "ALPACA_PROMPT = \"\"\"Aşağıda, bir görevi açıklayan bir talimat (instruction) ile daha fazla bağlam sağlayan bir girdi (input) bulunmaktadır. İsteği uygun şekilde tamamlayan bir yanıt (output) yazın.\n",
        "\n",
        "### Talimat:\n",
        "{}\n",
        "\n",
        "### Girdi:\n",
        "{}\n",
        "\n",
        "### Yanıt:\n",
        "{}\"\"\"\n",
        "\n",
        "@st.cache_resource\n",
        "def load_generator_model():\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = HF_REPO_NAME,\n",
        "        load_in_4bit = True,\n",
        "        dtype = None,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    return model, tokenizer\n",
        "\n",
        "@st.cache_resource\n",
        "def load_retriever():\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "    if not os.path.exists(FAISS_INDEX_PATH):\n",
        "        if not os.path.exists(PDF_PATH):\n",
        "             st.error(f\"HATA: '{PDF_PATH}' bulunamadı! Lütfen dosyayı yükleyin.\")\n",
        "             return None\n",
        "        loader = PyPDFLoader(PDF_PATH)\n",
        "        docs = loader.load()\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "        chunks = text_splitter.split_documents(docs)\n",
        "        vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "        vector_store.save_local(FAISS_INDEX_PATH)\n",
        "    else:\n",
        "        vector_store = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "    return vector_store\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"TCK Hukuk Asistanı\", layout=\"wide\")\n",
        "    st.title(\"⚖️ TCK Hukuk Asistanı (RAG + Fine-Tuned LLM)\")\n",
        "\n",
        "    # Model ve kütüphane yüklenirken kullanıcıyı bilgilendir\n",
        "    status_placeholder = st.empty()\n",
        "    status_placeholder.warning(\"⚠️ Model ve Kütüphaneler Hazırlanıyor, Lütfen Bekleyin...\")\n",
        "\n",
        "    vector_store = load_retriever()\n",
        "    model, tokenizer = load_generator_model()\n",
        "\n",
        "    # Her şey hazır olduğunda mesajı değiştir\n",
        "    status_placeholder.success(\"✅ Sistem Hazır! Sorunuzu sorabilirsiniz.\")\n",
        "\n",
        "    kullanici_sorusu = st.text_input(\"Sorunuzu yazın:\", placeholder=\"Zincirleme suç nedir?\")\n",
        "\n",
        "    if kullanici_sorusu:\n",
        "        with st.spinner(\"Cevap üretiliyor...\"):\n",
        "            docs = vector_store.similarity_search(kullanici_sorusu, k=3)\n",
        "            context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "            instruction = \"Verilen bağlamdaki kanun maddelerine sadık kalarak hukuk asistanı gibi cevap ver.\"\n",
        "            input_text = f\"BAĞLAM:\\n{context}\\n\\nKULLANICI SORUSU: {kullanici_sorusu}\"\n",
        "            prompt = ALPACA_PROMPT.format(instruction, input_text, \"\")\n",
        "\n",
        "            inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
        "            outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.3)\n",
        "            res = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "            answer = res.split(\"### Yanıt:\")[-1].strip()\n",
        "            st.markdown(\"### Asistanın Cevabı:\")\n",
        "            st.write(answer)\n",
        "            with st.expander(\"Referans Alınan Kanun Maddeleri (RAG)\"):\n",
        "                st.info(context)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGiprdP1Z7rU",
        "outputId": "b4a233cf-8ee8-4aea-c25e-6c2d1a7439fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Buraya kopyaladığın token'ı yapıştır\n",
        "ngrok.set_auth_token(\"33VAowwMmkNnVX0B68IWaSpRg92_3UiX9uS81aXoARFJJYiai\")\n",
        "\n",
        "# Streamlit'i arka planda başlat\n",
        "import os\n",
        "os.system(\"streamlit run app.py &\")\n",
        "\n",
        "# Tüneli aç\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Hukuk Asistanı Linki: {public_url.public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATe_Jq-vuY5e",
        "outputId": "bf27a928-be5a-47e9-9656-94af2cfbb931"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.5.0\n",
            "Hukuk Asistanı Linki: https://informational-principally-devora.ngrok-free.dev\n"
          ]
        }
      ]
    }
  ]
}